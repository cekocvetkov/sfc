---
title: "YOLOv11 Instance Segmentation with OpenCV and Java (Part 3)"
date: "2025-06-12"
tags: ["java", "opencv", "yolo", "python"]
image: "yolov11-seg-3/plane-seg.webp"
published: true
---

# YOLOv11 Instance Segmentation with OpenCV and Java (Part 3)

<Image
  placeholder="blur"
  blurDataURL="data:image/webp;base64,UklGRjAMAABXRUJQVlA4WAoAAAAgAAAANgMAOAMASUNDUMgBAAAAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADZWUDggQkoAABDTAJ0BKjcDOQM+7W6xVimlJCOgkHmZMB2JaW7hai8ccyT2S/Wc4YZhQB//945GPrxj9AAUG2opVMbailUWt2w9mbF7M2L3kLSqLYBaSiAdBBKIB0ORpjahXFXNRIDb0yjoYQHeUcuIc5oHEogHY+ZuMYjcsuZ3Twadit7hsYLopz7FdqjR5kCQBBQtVEsrrt32277dlkNJ+gABDvMhgx6wd2hgclCrxvqtzmQ9g9GKSXbvtt3Cdu4PzYbR152Zl6443wmg5Tp8b0JroG6jj4G6MDdGBKO3bz6V+jXqR0T4FViWLSVp2aNaVV9FZx3dCw3PpYX3wEW1Z+csWGF4eqcv/M3eoR3yXdo5IOCWDcFgGz9TTuE7d9OJR0ySglHYD94eqcvEXwpi2jOPGzgMAHYqdQoA3A2fqad99S5IDpnIfLQ7nGIHc5sk+n3vCoGNlst1pV1fYfLhWbMDdUK9+/1mExQrYh4RQO5xiB3OU2RSORWm5IjO+Z2fZLX322h7dlknVpA7mt8sPWu6py8PVOYLCf9IOfLtqJZ0JJ1TLHfYJdM3cH4gdIXwnnqlNsB/Ad4RQSxNwRBymiT5BKxajA0kCHhIItrbxzYeWOqcvD1T5"
  alt="Main image"
  src={"/images/yolov11-seg-3/plane-seg.webp"}
  width={0}
  height={0}
  sizes="100vw"
  style={{ width: "1400px", height: "335px", objectFit: "cover" }}
/>
<small className="w-full h-2 flex justify-center" style={{ marginTop: "-2rem" }}>
  you can also read this article in Medium -> click [here](https://medium.com/itnext/yolov11-instance-segmentation-with-opencv-and-java-part-3-95af00be325e)
</small>
<br />

In this article, we will look at instance segmentation using **YOLOv11**.

This is **Part 3** of a **3-part** series.

[Instance segmentation](https://www.ultralytics.com/glossary/instance-segmentation) goes a step further than object detection and involves identifying individual objects in an image and segmenting them from the rest of the image.

> In the [first part](https://itnext.io/yolov11-instance-segmentation-with-opencv-and-java-part-1-8180afb52313) and [second part](https://itnext.io/yolov11-instance-segmentation-with-opencv-and-java-part-2-582f57f8967a), we set up our project, prepared the input image for the model, loaded our model, and fed the input data through the network. We also had a look at the post-processing and extracted the segmentation information from the results. Now, we will see how to overlay the segmentation masks over the original image.

In this part, we'll inspect the `maskOutputs` in detail and perform a matrix multiplication between the 32 mask coefficients we extracted in the previous step and the 32 prototype masks in `maskOutputs`.

## Understanding the Mask Outputs

After we ran our input image through the network, we got two [Mat](https://docs.opencv.org/3.4/d3/d63/classcv_1_1Mat.html) output objects. The first one (`boxOutputs`) was the **Bounding Box Predictions**. We had a close look at them in the previous part. Here the code as a reminder:

```java
List<String> outNames = net.getUnconnectedOutLayersNames();
List<Mat> outputsList = new ArrayList<>();
net.forward(outputsList, outNames);

// Get relevant outputs and print them out
Mat boxOutputs = outputsList.get(0);
Mat maskOutputs = outputsList.get(1);

LOGGER.info("Boxes Output: "+boxOutputs.toString());
LOGGER.info("Masks Output: "+maskOutputs.toString());
```

The `boxOutputs` had a shape of **[1, 116, 8400]**, which we transposed to **[8400 x 116]** to make it easier to work with. This meant that for every 8400 predictions, we had 116 values with the following structure:

<Image
  alt="boxOutputs 116 values structure"
  src={"/images/yolov11-seg-3/row-struct.webp"}
  placeholder="blur"
  blurDataURL="data:image/webp;base64,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
"
  width={1400}
  height={400}
  objectFit="cover"
/>

1. **Bounding box predictions**
2. **Class probabilities**
3. **Mask coefficients** (which we extracted as a last step in the previous part).

Now let's concentrate on the masks output (`maskOutputs`):

```
INFO: Masks Output: Mat [ 1*32*160*160*CV_32FC1, isCont=true, isSubmat=false, nativeObj=0x600000b57d20, dataAddr=0x14abf8000 ]
```

We see that it has a shape of **[1, 32, 160, 160]**. Let's break it down:

The `maskOutputs` contains **32 masks**, each of size **160x160 pixels**. (1 is just the batch size — meaning, we're processing one image).

### Now we get to the main part:
> **Instance segmentation works by taking each of these 32 masks and multiplying it by its corresponding mask coefficient from the boxOutputs.**

For example — if the first mask coefficient is 0.3, we multiply every pixel in the first mask by 0.3. If the second coefficient is 0.9, we multiply every pixel in the second mask by 0.9, and so on. We'll do this for all 32 masks and then sum all the resulting masks pixel by pixel to create one combined mask. This is called **linear combination** and this is what is going to give us the final segmentation mask.

Let's have a look at the code snippet we inspected the last time:

```java
var segmentationMasks = new ArrayList<Mat>();

LOGGER.info("-----Start analysing the inference-----");
for (int i = 0; i < mat2D.rows(); i++)
{
    Mat detectionMat = mat2D.row(i);
    List<Double> scores = new ArrayList<>();
    for (int j = 4; j < NUM_CLASSES+4; j++) {
        scores.add(mat2D.get(i, j)[0]);
    }

    MaxScore maxScore = ScoreUtils.findMaxScore( scores );
    if(maxScore.maxValue() < 0.6) {
        continue;
    }

    // Extract mask coefficients
    Mat maskCoeffs = detectionMat.colRange(4 + NUM_CLASSES, 4 + NUM_CLASSES + 32);
    // Generate mask for this detection
    Mat objectMask = generateMask(maskOutputs, maskCoeffs);
    segmentationMasks.add(objectMask);
}
```

We'll have a closer look at one of the last lines there:
```java
Mat objectMask = generateMask(maskOutputs, maskCoeffs);
```

## Generating the Final Mask

We just talked about how we can use the `maskOutputs` and the `maskCoefficients` to generate the final mask, so let's have a look at the actual implementation of the `generateMask` method:

```java
private static Mat generateMask(Mat prototypesMasks, Mat maskCoeffs) {
    Mat mask = new Mat(MASK_RESOLUTION, MASK_RESOLUTION, CvType.CV_32F, new Scalar(0));

    for (int i = 0; i < 32; i++) {
        float coeff = (float) maskCoeffs.get(0, i)[0];

        Mat prototypeMask = prototypesMasks.col(i).reshape(1, MASK_RESOLUTION);
        Core.addWeighted(mask, 1.0, prototypeMask, coeff, 0.0, mask);
    }

    // Apply sigmoid to get final mask (σ(x) = 1 / (1 + e^(-x)))
    Mat floatMask = new Mat();
    mask.convertTo(floatMask, CvType.CV_32F);
    Core.multiply(floatMask, new Scalar(-1.0), floatMask);// -x
    Core.exp(floatMask, floatMask); // e^(-x)
    Core.add(floatMask, new Scalar(1.0), floatMask); // 1 + e^(-x)
    Core.divide(1.0, floatMask, floatMask); // 1 / (1 + e^(-x))

    return floatMask;
}
```

`MASK_RESOLUTION` is 160, as we saw above, I've just added it as a constant.

Let's have a closer look at what happens here:

1. **We create a blank mask and iteratively add all the 32 prototype masks, each weighted by its corresponding coefficient.** We use OpenCV's `addWeighted` function here to do the weighted addition of each prototype mask. On every iteration, we add the `prototypeMask × coeff` to the `mask` Mat, which in the end represents our final mask.
   (the fifth parameter 0.0 is the gamma parameter here, which is a scalar value that can be added to the result)

2. **The final mask can contain values outside the typical [0,1] range, which is expected for masks, so to address this, we apply the sigmoid activation function as a post-processing step:**
   ```
   σ(x) = 1/(1 + e^(-x))
   ```
   This normalizes all the pixel values to the range [0,1]

Now that we have all the prototype masks in the `segmentationMasks` list!

## Overlaying the Mask on the Original Image

Let's simplify our logic a bit and assume we only have one object for segmentation in our image (like my plane example in the main article image).

All the masks we get are going to be very similar so we can just take one of them and apply it on our image to see the result:

```java
Mat objectMask = segmentationMasks.get(0);
```

> Note again that this only works assuming we have one object in the image

### Now let's overlay this mask on top of our image and visualize the instance segmentation.

What we need to do next is the following:

1. **Resizes the mask to match the dimensions of the original image**
2. **Converts the mask from floating-point values to the full 0–255 pixel range.**
3. **Use threshold to make all object pixels white.** Let's say we choose threshold 200: pixels ≥ 200 become 255 (white), pixels < 200 become 0 (black)
4. **Apply pink overlay to the original image where the mask is active.**

Here the code:

```java
Mat objectMask = segmentationMasks.getFirst();
Mat result = image.clone();

Mat resizedMask = resizeMask(objectMask, image);

// here we just get black and white (object is in white)
Mat normalizedMask = normalizeMask(resizedMask);

// Use threshold to make all object pixels white
Imgproc.threshold(normalizedMask, normalizedMask, 200, 255, Imgproc.THRESH_BINARY);
// Create a solid pink colored image with the same size of our result image
Mat pinkLayer = new Mat(result.size(), result.type(), PINK );
//Copies pink pixels from pinkLayer to result only where normalizedMask is white (255)
pinkLayer.copyTo(result, normalizedMask);

Imgproc.resize(result, result, new Size(IMG_SIZE, IMG_SIZE), 0, 0, Imgproc.INTER_LINEAR);

HighGui.imshow( "results", result );
HighGui.waitKey(0);

HighGui.destroyAllWindows();
System.exit(0);
```

`IMG_SIZE` is 640 (see Part 1):

> `IMG_SIZE` is 640px for this YOLO model

And the `PINK` constant is `new Scalar(203, 192, 255)`

The methods for resizing and normalization are pretty simple too:

```java
private static Mat resizeMask(Mat mask,Mat originalImage) {
    Mat resized = new Mat();
    Imgproc.resize(mask, resized, originalImage.size(), 0, 0, Imgproc.INTER_LINEAR);
    LOGGER.info("Resized mask size: " + resized.size());
    return resized;
}

private static Mat normalizeMask(Mat mask) {
    Mat normalized = new Mat();
    Core.normalize(mask, normalized, 0, 255, Core.NORM_MINMAX, CvType.CV_8U);
    return normalized;
}
```

I hope you managed to follow along. If you run the program now, you'll see the result!

<Image
  alt="Final Result"
  src={"/images/yolov11-seg-3/plane-seg.webp"}
  placeholder="blur"
  blurDataURL="data:image/webp;base64,UklGRmgIAABXRUJQVlA4WAoAAAAgAAAANgMAgwIASUNDUMgBAAAAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADZWUDggeAYAALCVAJ0BKjcDhAI+7Xa0ViklJKOgCLkgHYlpbuFrb66HF8M4x30ACAfPwvkgEbQIBPSgAOtpjAwbGqVTGBg3r4Y3p6NmtBKaYyS7G4g2xQq44eI12MvFiK0PsxqMg3p6djJ0HUZHOm3hKhgZhSFKYLXPEKBiSXhJYFy7pJK9JOgC3ygQKTJ+Wf2z6QzLrxeOGVyEMxKJSdz0EKe3zI9J9I8vJiYHm5AjYuXdJJ+DJV9AJ4pJQogJ4pI5oAALASxnpDDQYyK7ACh3Kgz5k8ck2voCgCYGOZ7iB9FLNaHPqT8l4LpPdkdJD4gK8cksG1QkMIz2vSCCiYnGEIcQ9mAwJE9mfwkXLbcBMgJJFFFfNkNyTwAAAA="
  width={1400}
  height={400}
  objectFit="cover"
/>

<br />
<div className="text-2xl"> Perfect! We've successfully completed instance segmentation with YOLOv11! </div>

## Conclusion

Hopefully this series helped you get a grasp of how to do instance segmentation in Java with the help of OpenCV and YOLOv11. The process is not very straight-forward, but I hope we managed to tackle all the tricky parts in detail. If you have any questions, I'll be happy to help!

Java is becoming better and better when it comes to rapidly experimenting with object detection, instance segmentation, and integration with LLMs. This series demonstrates what is now possible in this ecosystem with the help of OpenCV.

**Thanks for reading!**

<div className="h-10 w-2"></div>
